{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "\n",
    "I started the data wrangling process by gathering data from three sources.I downloaded a CSV file from the Udacity website and read it into a dataframe called **tweet_archive**. The second data was downloaded from a URL provided by Udacity using requests library and wrote it to a TSV file . I then read this file into dataframe called **image_predictions**. Thirdly, l gathered additional data via Twitter's API. After saving each tweet's returned JSON as a new line in a .txt file, I was able to read it line by line using the `json` library and created a dataframe called **tweet_data**.\n",
    "\n",
    "Thereafter, I performed visual and programmatic assessment of the data. In the tweet_archive dataframe, the issues mainly consisted of, among others, incorrect extraction of ratings and dog stages from the text column. In addition, data from image_predictions dataframe was untidy, as there were many predictions and only one was enough. In the tweet_data dataframe, some tweets were deleted and it was impossible to retrieve them. There was existence of incorrect datatypes, inconsistent capitalisation of dog names,status and breed, and content without images and including non dog ratings in dataframes.\n",
    "\n",
    "Starting with creating copies of the dataframe **(archive_clean, predictions_clean and tweets_clean)**, I commenced the iterative cleaning process of defining the cleaning action, coding and then test the result. To narrow down the dataframes, only original content with images and tweets about dog ratings was included. Additionally, I removed uninterested columns, changed incorrect data types, made capitalization of column values from each dataset to be consistent. l also extracted correct ratings and dog status information from text column. This part was the most complex of the cleaning process, and on that note, there might be some issues with archive_clean, as it was impossible to go through each tweet individually. I narrowed down each dataframe to only original content with images, as well as tidy the data by having one dataframe for each observational unit. Finally, l resolved any other remaining quality issues.\n",
    "\n",
    "To end the data wrangling process, I combined all the three clean datasets into one clean csv file called **twitter_archive_master**. Lastly, a simple analysis and visualization of wrangled data was then carried out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
